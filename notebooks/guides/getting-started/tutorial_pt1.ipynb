{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQL_5twfli0L"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/shareproduct123/cohere_notebooks/blob/main/notebooks/guides/getting-started/tutorial_pt1.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1qYsdXAli0O"
      },
      "source": [
        "# Cohere Tutorial\n",
        "\n",
        "#### Build your first Cohere application: An onboarding assistant for new hires"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4eLgy1nli0P"
      },
      "source": [
        "Welcome to the Cohere tutorial – a hands-on introduction to Cohere!\n",
        "\n",
        "In this tutorial, you will learn how to use the Cohere API, specifically three endpoints: Chat, Embed, and Rerank.\n",
        "\n",
        "This tutorial is split over seven parts, with each part focusing on one use case, as follows:\n",
        "\n",
        "- Part 1: Installation and Setup (Pre-requisite)\n",
        "- Part 2: Text Generation\n",
        "- Part 3: Chatbots\n",
        "- Part 4: Semantic Search\n",
        "- Part 5: Reranking\n",
        "- Part 6: Retrieval-Augmented Generation (RAG)\n",
        "- Part 7: Agents with Tool Use\n",
        "\n",
        "You'll learn about these use cases by building an onboarding assistant that helps new hires onboard to a fictitious company called Co1t. The assistant can help write introductions, answer user questions about the company, search for information from e-mails, and create meeting appointments.\n",
        "\n",
        "We recommend that you follow the parts sequentially. However, feel free to skip to specific parts if you want (apart from Part 1, which is a pre-requisite) because each part also works as a standalone tutorial.\n",
        "\n",
        "Total Duration: ~15 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGZSmmaDli0S"
      },
      "source": [
        "## Installation and Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRuvKfZBli0T"
      },
      "source": [
        "The Cohere platform lets developers access large language model (LLM) capabilities with a few lines of code. These LLMs can solve a broad spectrum of natural language use cases, including classification, semantic search, paraphrasing, summarization, and content generation.\n",
        "\n",
        "Cohere's models can be accessed through the playground, SDK, and CLI tool. We support SDKs in four different languages: Python, Typescript, Java, and Go.\n",
        "\n",
        "This tutorial uses the Python SDK and accesses the models through the Cohere platform.\n",
        "\n",
        "To get started, first install the Cohere Python SDK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdocOJUPli0U",
        "outputId": "b9b3a7d4-3b51-42e5-e3ca-583a8f95685f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.13.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.27.2)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.9.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.23.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.20.3)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.2.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.26.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Downloading cohere-5.13.0-py3-none-any.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.7/249.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: types-requests, parameterized, httpx-sse, fastavro, cohere\n",
            "Successfully installed cohere-5.13.0 fastavro-1.9.7 httpx-sse-0.4.0 parameterized-0.9.0 types-requests-2.32.0.20241016\n"
          ]
        }
      ],
      "source": [
        "!pip install cohere --upgrade # Upgrade to the latest version of cohere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uZcvEdLli0W"
      },
      "source": [
        "Next, we'll import the `cohere` library and create a client to be used throughout the examples. We create a client by passing the Cohere API key as an argument. To get an API key, [sign up with Cohere](https://dashboard.cohere.com/welcome/register) and get the API key [from the dashboard](https://dashboard.cohere.com/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfoURY4rli0X",
        "outputId": "d8d70f7b-4085-4a5f-e31c-b2f90395d00a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<cohere.client_v2.ClientV2 at 0x79735dc11870>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import cohere\n",
        "\n",
        "co = cohere.ClientV2(api_key=\"1GLAqzgbTVGbUTfUeblk3ByYtQyN0fjvHLbs5fPr\") # Get your API key here: https://dashboard.cohere.com/api-keys\n",
        "co\n",
        "\n",
        "# Tivaly Key: \"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\"   \"RtM9PLbik0060hmsY6hv8pwoVJKBBojafBgvL8x2 \""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j42VMfTPli0Y"
      },
      "source": [
        "# Accessing Cohere from Other Platforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QICXw3cvli0Z"
      },
      "source": [
        "The Cohere platform is the fastest way to access Cohere's models and get started.\n",
        "\n",
        "However, if you prefer other options, you can access Cohere's models through other platforms such as Amazon Bedrock, Amazon SageMaker, Azure AI Studio, and Oracle Cloud Infrastructure (OCI) Generative AI Service.\n",
        "\n",
        "Read this documentation on [Cohere SDK cloud platform compatibility](https://docs.cohere.com/docs/cohere-works-everywhere)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEX2ygk5li0a"
      },
      "source": [
        "## Amazon Bedrock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIT_os85li0a"
      },
      "source": [
        "The following is how you can create a Cohere client on Amazon Bedrock.\n",
        "\n",
        "For further information, read this documentation on [Cohere on Bedrock](https://docs.cohere.com/docs/cohere-on-aws#amazon-bedrock)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFRrnUpQli0b"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "\n",
        "co = cohere.BedrockClient(\n",
        "    aws_region=\"...\",\n",
        "    aws_access_key=\"...\",\n",
        "    aws_secret_key=\"...\",\n",
        "    aws_session_token=\"...\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je88qqXzli0c"
      },
      "source": [
        "## Amazon SageMaker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhQB3Y9xli0c"
      },
      "source": [
        "The following is how you can create a Cohere client on Amazon SageMaker.\n",
        "\n",
        "For further information, read this documentation on [Cohere on SageMaker](https://docs.cohere.com/docs/cohere-on-aws#amazon-sagemaker)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liw5vCcXli0d"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "\n",
        "co = cohere.SagemakerClient(\n",
        "    aws_region=\"us-east-1\",\n",
        "    aws_access_key=\"...\",\n",
        "    aws_secret_key=\"...\",\n",
        "    aws_session_token=\"...\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYWtPcQpli0d"
      },
      "source": [
        "## Microsoft Azure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcFEUOxcli0e"
      },
      "source": [
        "The following is how you can create a Cohere client on Microsoft Azure.\n",
        "\n",
        "For further information, read this documentation on [Cohere on Azure](https://docs.cohere.com/docs/cohere-on-microsoft-azure)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKOpCsRMli0e"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "\n",
        "co = cohere.Client(\n",
        "  api_key=\"...\",\n",
        "  base_url=\"...\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Fo7_ppli0f"
      },
      "source": [
        "In Part 2, we'll get started with the first use case - text generation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4n3_BxTCzy1l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASnI8nxyzBi4"
      },
      "source": [
        "# Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYku-iF2zBi5"
      },
      "source": [
        "Command is Cohere’s flagship LLM. It generates a response based on a user message or prompt. It is trained to follow user commands and to be instantly useful in practical business applications, like summarization, copywriting, extraction, and question-answering.\n",
        "\n",
        "Command R and Command R+ are the most recent models in the Command family. They are the market-leading models that balance high efficiency with strong accuracy to enable enterprises to move from proof of concept into production-grade AI.\n",
        "\n",
        "You'll use Chat, the Cohere endpoint for accessing the Command models.\n",
        "\n",
        "In this tutorial, you'll learn about:\n",
        "- Basic text generation\n",
        "- Prompt engineering\n",
        "- Parameters for controlling output\n",
        "- Structured output generation\n",
        "- Streamed output\n",
        "\n",
        "You'll learn these by building an onboarding assistant for new hires."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA6u001LzBi7"
      },
      "source": [
        "## Basic text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-dSAHQXzBi8"
      },
      "source": [
        "To get started with Chat, we need to pass two parameters, `model` for the LLM model ID and `messages`, which we add a single user message. We then call the Chat endpoint through the client we created earlier.\n",
        "\n",
        "The response contains several objects. For simplicity, what we want right now is the `message.content[0].text` object.\n",
        "\n",
        "Here's an example of the assistant responding to a new hire's query asking for help to make introductions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fgZnpGwQzBi9",
        "outputId": "5c5ec749-718d-4147-e772-db8200c3a775",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a draft of an introduction message for your first day at Co1t:\n",
            "\n",
            "\"Hello everyone!\n",
            "\n",
            "My name is [Your Name], and I am thrilled to join the Co1t family as a new team member! Starting today, I will be working with all of you, and I couldn't be more excited to contribute to this innovative startup.\n",
            "\n",
            "A little about myself: I have a background in [Your Educational or Professional Field] and have always been passionate about [Relevant Skills or Interests]. I believe my experience in [Specific Skills or Projects] will be valuable in helping Co1t achieve its goals. I'm looking forward to learning from all of you and sharing my insights as well.\n",
            "\n",
            "I'm excited to collaborate, brainstorm, and tackle the challenges ahead as a team. Feel free to reach out if you'd like to connect and chat further. Let's make some amazing things happen together!\n",
            "\n",
            "Cheers,\n",
            "[Your Name]\"\n",
            "\n",
            "Feel free to customize and add more personal details to make the introduction more engaging and reflective of your personality! Good luck on your first day, and congratulations on joining the new venture!\n"
          ]
        }
      ],
      "source": [
        "# Add the user message\n",
        "message = \"I'm joining a new startup called Co1t today. Could you help me write a short introduction message to my teammates.\"\n",
        "'''\n",
        "# Generate the response\n",
        "# Use generate with prompt instead of chat with messages\n",
        "response = co.generate(\n",
        "    model=\"command-r-plus-08-2024\",\n",
        "    prompt=message,\n",
        "    max_tokens=200, # adjust as needed\n",
        ")\n",
        "\n",
        "'''\n",
        "response = co.chat(\n",
        "    model=\"command-r-plus-08-2024\",\n",
        "    messages=[{\"role\": \"user\", \"content\": message}],\n",
        ")\n",
        "\n",
        "print(response.message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83bmxRJbzBi-"
      },
      "source": [
        "Further reading:\n",
        "- [Chat endpoint API reference](https://docs.cohere.com/v2/reference/chat)\n",
        "- [Documentation on Chat fine-tuning](https://docs.cohere.com/docs/chat-fine-tuning)\n",
        "- [Documentation on Command R+](https://docs.cohere.com/docs/command-r-plus)\n",
        "- [LLM University module on text generation](https://cohere.com/llmu#text-generation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj2XwYl0zBi_"
      },
      "source": [
        "## Prompt engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgIH7yS7zBi_"
      },
      "source": [
        "Prompting is at the heart of working with LLMs. The prompt provides context for the text that we want the model to generate. The prompts we create can be anything from simple instructions to more complex pieces of text, and they are used to encourage the model to produce a specific type of output.\n",
        "\n",
        "In this section, we'll look at a couple of prompting techniques.\n",
        "\n",
        "The first is to add more specific instructions to the prompt. The more instructions you provide in the prompt, the closer you can get to the response you need.\n",
        "\n",
        "The limit of how long a prompt can be is dependent on the maximum context length that a model can support (in the case Command R/R+, it's 128k tokens).\n",
        "\n",
        "Below, we'll add one additional instruction to the earlier prompt: the length we need the response to be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B0qyS-1SzBjA",
        "outputId": "90d9b002-fa5f-499d-89bb-ef3d614e7df1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Excited to be on board as the newest member of Co1t, I'm [Your Name], a [Your Role/Position], eager to contribute my skills and collaborate with this talented team to drive innovation and success.\"\n"
          ]
        }
      ],
      "source": [
        "# Add the user message\n",
        "message = \"I'm joining a new startup called Co1t today. Could you help me write a one-sentence introduction message to my teammates.\"\n",
        "\n",
        "# Generate the response\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=[{\"role\": \"user\", \"content\": message}])\n",
        "                #    messages=[cohere.UserMessage(content=message)])\n",
        "\n",
        "print(response.message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMquY0HozBjA"
      },
      "source": [
        "All our prompts so far use what is called zero-shot prompting, which means that provide instruction without any example. But in many cases, it is extremely helpful to provide examples to the model to guide its response. This is called few-shot prompting.\n",
        "\n",
        "Few-shot prompting is especially useful when we want the model response to follow a particular style or format. Also, it is sometimes hard to explain what you want in an instruction, and easier to show examples.\n",
        "\n",
        "Below, we want the response to be similar in style and length to the convention, as we show in the examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "94GRxgqPzBjA",
        "outputId": "a1738385-2e3b-4a24-ec2a-a54b217403e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server Access Denied: Possible Permissions Issue\n"
          ]
        }
      ],
      "source": [
        "# Add the user message\n",
        "user_input = \"Why can't I access the server? Is it a permissions issue?\"\n",
        "\n",
        "# Create a prompt containing example outputs\n",
        "message=f\"\"\"Write a ticket title for the following user request:\n",
        "\n",
        "User request: Where are the usual storage places for project files?\n",
        "Ticket title: Project File Storage Location\n",
        "\n",
        "User request: Emails won't send. What could be the issue?\n",
        "Ticket title: Email Sending Issues\n",
        "\n",
        "User request: How can I set up a connection to the office printer?\n",
        "Ticket title: Printer Connection Setup\n",
        "\n",
        "User request: {user_input}\n",
        "Ticket title:\"\"\"\n",
        "\n",
        "# Generate the response\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=[{\"role\": \"user\", \"content\": message}])\n",
        "\n",
        "print(response.message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2xuw9BOzBjB"
      },
      "source": [
        "Further reading:\n",
        "- [Documentation on prompt engineering](https://docs.cohere.com/docs/crafting-effective-prompts)\n",
        "- [LLM University module on prompt engineering](https://cohere.com/llmu#prompt-engineering)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vq6rX2yzBjB"
      },
      "source": [
        "## Parameters for controlling output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3sZhB-izBjB"
      },
      "source": [
        "The Chat endpoint provides developers with an array of options and parameters.\n",
        "\n",
        "For example, you can choose from several variations of the Command model. Different models produce different output profiles, such as quality and latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ujP1lLfGzBjC",
        "outputId": "b55f0a12-c483-4525-edb6-7ac0ded46932",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Excited to be on board as the newest member of Co1t, I'm [Your Name], a [Your Role/Position], eager to contribute my skills and collaborate with this talented team!\"\n"
          ]
        }
      ],
      "source": [
        "# Add the user message\n",
        "message = \"I'm joining a new startup called Co1t today. Could you help me write a one-sentence introduction message to my teammates.\"\n",
        "\n",
        "# Generate the response\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=[{\"role\": \"user\", \"content\": message}])\n",
        "\n",
        "print(response.message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNM7qZzfzBjC"
      },
      "source": [
        "Often, you’ll need to control the level of randomness of the output. You can control this using a few parameters.\n",
        "\n",
        "The most commonly used parameter is `temperature`, which is a number used to tune the degree of randomness. You can enter values between 0.0 to 1.0.\n",
        "\n",
        "A lower temperature gives more predictable outputs, and a higher temperature gives more \"creative\" outputs.\n",
        "\n",
        "Here's an example of setting `temperature` to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "c7pZd29UzBjD",
        "outputId": "1b5238cf-1a82-434c-9525-ec680618f39e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: History buff, curious about industrialization.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TooManyRequestsError",
          "evalue": "status_code: 429, body: data=None message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTooManyRequestsError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-57ecab56b25b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Generate the response multiple times by specifying a low temperature value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     response = co.chat(model=\"command-r-plus-08-2024\",\n\u001b[0m\u001b[1;32m      7\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     temperature=0)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cohere/client.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m\"To suppress this warning, set `log_warning_experimental_features=False` when initializing the client.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             )\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_async_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cohere/client.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mcheck_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_async_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/cohere/v2/client.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self, model, messages, tools, strict_tools, documents, citation_options, response_format, safety_mode, max_tokens, stop_sequences, temperature, seed, frequency_penalty, presence_penalty, k, p, return_prompt, logprobs, request_options)\u001b[0m\n\u001b[1;32m    634\u001b[0m                 )\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m429\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m                 raise TooManyRequestsError(\n\u001b[0m\u001b[1;32m    637\u001b[0m                     typing.cast(\n\u001b[1;32m    638\u001b[0m                         \u001b[0mTooManyRequestsErrorBody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTooManyRequestsError\u001b[0m: status_code: 429, body: data=None message=\"You are using a Trial key, which is limited to 10 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions\""
          ]
        }
      ],
      "source": [
        "# Add the user message\n",
        "message = \"I like learning about the industrial revolution and how it shapes the modern world. How I can introduce myself in five words or less.\"\n",
        "\n",
        "# Generate the response multiple times by specifying a low temperature value\n",
        "for idx in range(3):\n",
        "    response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                    messages=[{\"role\": \"user\", \"content\": message}],\n",
        "                    temperature=0)\n",
        "\n",
        "    print(f\"{idx+1}: {response.message.content[0].text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZNqIc5KzBjD"
      },
      "source": [
        "And here's an example of setting `temperature` to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9WDw9K15zBjD",
        "outputId": "bb16a45d-52db-4bc0-abeb-e3bc91656238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: \"I explore history's revolutionary impact.\"\n",
            "\n",
            "2: Hi, I'm an Industrial Revolution enthusiast.\n",
            "\n",
            "3: History enthusiast, Industrial Revolution lover.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Add the user message\n",
        "message = \"I like learning about the industrial revolution and how it shapes the modern world. How I can introduce myself in five words or less.\"\n",
        "\n",
        "# Generate the response multiple times by specifying a low temperature value\n",
        "for idx in range(3):\n",
        "    response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                    messages=[{\"role\": \"user\", \"content\": message}],\n",
        "                    temperature=1)\n",
        "\n",
        "    print(f\"{idx+1}: {response.message.content[0].text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz2HIJtjzBjE"
      },
      "source": [
        "Further reading:\n",
        "- [Available models for the Chat endpoint](https://docs.cohere.com/docs/models#command)\n",
        "- [Documentation on predictable outputs](https://docs.cohere.com/v2/docs/predictable-outputs)\n",
        "- [Documentation on advanced generation parameters](https://docs.cohere.com/docs/advanced-generation-hyperparameters)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsHsiBs5zBjE"
      },
      "source": [
        "## Structured output generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUXJltFnzBjE"
      },
      "source": [
        "By adding the `response_format` parameter, you can get the model to generate the output as a JSON object. By generating JSON objects, you can structure and organize the model's responses in a way that can be used in downstream applications.\n",
        "\n",
        "The `response_format` parameter allows you to specify the schema the JSON object must follow. It takes the following parameters:\n",
        "- `message`: The user message\n",
        "- `response_format`: The schema of the JSON object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1DF8Cov0zBjE",
        "outputId": "d6043831-cac5-4748-fc74-cd184162eb3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cohere.client:The `response_format.schema` parameter is an experimental feature and may change in future releases.\n",
            "To suppress this warning, set `log_warning_experimental_features=False` when initializing the client.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'Server Access Issue', 'category': 'access', 'status': 'open'}\n"
          ]
        }
      ],
      "source": [
        "import json # import the json module\n",
        "# Add the user message\n",
        "user_input = \"Why can't I access the server? Is it a permissions issue?\"\n",
        "message = f\"\"\"Create an IT ticket for the following user request. Generate a JSON object.\n",
        "{user_input}\"\"\"\n",
        "\n",
        "# Generate the response multiple times by adding the JSON schema\n",
        "response = co.chat(\n",
        "  model=\"command-r-plus-08-2024\",\n",
        "  messages=[{\"role\": \"user\", \"content\": message}],\n",
        "  response_format={\n",
        "    \"type\": \"json_object\",\n",
        "    \"schema\": {\n",
        "      \"type\": \"object\",\n",
        "      \"required\": [\"title\", \"category\", \"status\"],\n",
        "      \"properties\": {\n",
        "        \"title\": { \"type\": \"string\"},\n",
        "        \"category\": { \"type\" : \"string\", \"enum\" : [\"access\", \"software\"]},\n",
        "        \"status\": { \"type\" : \"string\" , \"enum\" : [\"open\", \"closed\"]}\n",
        "      }\n",
        "    }\n",
        "  },\n",
        ")\n",
        "\n",
        "json_object = json.loads(response.message.content[0].text)\n",
        "\n",
        "print(json_object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSVLolhxzBjF"
      },
      "source": [
        "Further reading:\n",
        "- [Documentation on Structured Generations (JSON)](https://docs.cohere.com/docs/structured-outputs-json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asP_-s0gzBjF"
      },
      "source": [
        "## Streaming responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck446u60zBjF"
      },
      "source": [
        "All the previous examples above generate responses in a non-streamed manner. This means that the endpoint would return a response object only after the model has generated the text in full.\n",
        "\n",
        "The Chat endpoint also provides streaming support. In a streamed response, the endpoint would return a response object for each token as it is being generated. This means you can display the text incrementally without having to wait for the full completion.\n",
        "\n",
        "To activate it, use `co.chat_stream()` instead of `co.chat()`.\n",
        "\n",
        "In streaming mode, the endpoint will generate a series of objects. To get the actual text contents, we take objects whose `event_type` is `content-delta`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "p_75TVJ8zBjG",
        "outputId": "50a2c8da-d62f-4ece-de91-626992f49459",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Excited to be on board as the newest member of Co1t, I'm [Your Name], a [Your Role/Position], eager to contribute my skills and collaborate with this talented team to drive innovation and success.\""
          ]
        }
      ],
      "source": [
        "# Add the user message\n",
        "message = \"I'm joining a new startup called Co1t today. Could you help me write a one-sentence introduction message to my teammates.\"\n",
        "\n",
        "# Generate the response by streaming it\n",
        "response = co.chat_stream(model=\"command-r-plus-08-2024\",\n",
        "                          messages=[{\"role\": \"user\", \"content\": message}])\n",
        "\n",
        "for event in response:\n",
        "    if event:\n",
        "        if event.type == \"content-delta\":\n",
        "            print(event.delta.message.content.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGRQArA8zBjG"
      },
      "source": [
        "Further reading:\n",
        "- [Documentation on streaming responses](https://docs.cohere.com/docs/streaming)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zup-owALzBjG"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j99KYJjpzBjH"
      },
      "source": [
        "In this tutorial, you learned about:\n",
        "- How to get started with a basic text generation\n",
        "- How to improve outputs with prompt engineering\n",
        "- How to control outputs using parameter changes\n",
        "- How to generate structured outputs\n",
        "- How to stream text generation outputs\n",
        "\n",
        "However, we have only done all this using direct text generations. As its name implies, the Chat endpoint can also support building chatbots, which require features to support multi-turn conversations and maintain the conversation state.\n",
        "\n",
        "In Part 3, you'll learn how to build chatbots with the Chat endpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KcHVHIe3LPr"
      },
      "source": [
        "# Chatbots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68h73r-e3LPs"
      },
      "source": [
        "As its name implies, the Chat endpoint enables developers to build chatbots that can handle conversations. At the core of a conversation is a multi-turn dialog between the user and the chatbot. This requires the chatbot to have the state (or “memory”) of all the previous turns to maintain the state of the conversation.\n",
        "\n",
        "In this tutorial, you'll learn about:\n",
        "- Creating a custom preamble\n",
        "- Creating a single-turn conversation\n",
        "- Building the conversation memory\n",
        "- Running a multi-turn conversation\n",
        "- Viewing the chat history\n",
        "\n",
        "You'll learn these by building an onboarding assistant for new hires."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-fAkglu3LPt"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To get started, first we need to install the `cohere` library and create a Cohere client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mUX86EeZ3LPu"
      },
      "outputs": [],
      "source": [
        "# pip install cohere\n",
        "\n",
        "#import cohere\n",
        "\n",
        "#co = cohere.ClientV2(api_key\"COHERE_API_KEY\") # Get your free API key: https://dashboard.cohere.com/api-keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZww2nrV3LPu"
      },
      "source": [
        "## Creating a custom preamble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miC0_ujY3LPv"
      },
      "source": [
        "A conversation starts with a system message, or a preamble, to help steer a chatbot’s response toward certain characteristics.\n",
        "\n",
        "For example, if we want the chatbot to adopt a formal style, the preamble can be used to encourage the generation of more business-like and professional responses.\n",
        "\n",
        "The recommended approach is to use two H2 Markdown headers: \"Task and Context\" and \"Style Guide\" in the exact order.\n",
        "\n",
        "In the example below, the preamble provides context for the assistant's task (task and context) and encourages the generation of rhymes as much as possible (style guide)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7Ht1vgZ_3LPv",
        "outputId": "2b613cc0-c137-450f-910f-26f96a3bc3e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a little note, a fun way to say hello,\n",
            "To your new colleagues, a warm welcome you'll bestow:\n",
            "\n",
            "\"Hello, fellow Co1t crew, a new adventure awaits!\n",
            "I'm thrilled to join and eager to collaborate.\n",
            "My name is [Your Name], a [Your Role] here to contribute,\n",
            "With skills and enthusiasm, I'm ready to disseminate.\n",
            "\n",
            "Let's connect and create, and together we'll innovate,\n",
            "Exciting times ahead, a journey we'll navigate!\"\n",
            "\n",
            "A fun rhyme to break the ice,\n",
            "A great team player, you'll be nice.\n"
          ]
        }
      ],
      "source": [
        "# Add the user message\n",
        "message = \"I'm joining a new startup called Co1t today. Could you help me write a short introduction message to my teammates.\"\n",
        "\n",
        "# Create a custom system message\n",
        "system_message=\"\"\"## Task and Context\n",
        "You are an assistant who assist new employees of Co1t with their first week.\n",
        "\n",
        "## Style Guide\n",
        "Try to speak in rhymes as much as possible. Be professional.\"\"\"\n",
        "\n",
        "# Add the messages\n",
        "messages = [{\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "# Generate the response\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=messages)\n",
        "\n",
        "print(response.message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVVikeH53LPx"
      },
      "source": [
        "Further reading:\n",
        "- [Documentation on preambles](https://docs.cohere.com/docs/preambles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWkLlYOG3LPx"
      },
      "source": [
        "## Starting the first conversation turn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyWtaIaJ3LPy"
      },
      "source": [
        "Let's start with the first conversation turn.\n",
        "\n",
        "Here, we are also adding a custom preamble or system message for generating a concise response, just to keep the outputs brief for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "np69-EIp3LPy",
        "outputId": "9e62c929-b085-41a7-d330-e260fb3b2563",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Hi everyone! I'm thrilled to join Co1t today and look forward to collaborating with this talented team and contributing to our startup's success.\"\n"
          ]
        }
      ],
      "source": [
        "# Add the user message\n",
        "message = \"I'm joining a new startup called Co1t today. Could you help me write a short introduction message to my teammates.\"\n",
        "\n",
        "# Create a custom system message\n",
        "system_message=\"\"\"## Task and Context\n",
        "Generate concise responses, with maximum one-sentence.\"\"\"\n",
        "\n",
        "# Add the messages\n",
        "messages = [{\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "# Generate the response\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=messages)\n",
        "\n",
        "print(response.message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4i5bJoq3LPz"
      },
      "source": [
        "## Building the conversation memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LGNw4wx3LPz"
      },
      "source": [
        "Now, we want the model to refine the earlier response. This requires the next generation to have access to the state, or memory, of the conversation.\n",
        "\n",
        "To do this, we append the `messages` with the model's previous response using the `assistant` role.\n",
        "\n",
        "Next, we also append a new user message (for the second turn) to the `messages` list.\n",
        "\n",
        "Looking at the response, we see that the model is able to get the context from the chat history. The model is able to capture that \"it\" in the user message refers to the introduction message it had generated earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qQpRfkI73LPz",
        "outputId": "f32c2eb1-1e9d-44e1-eadb-c2a087c5e26b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': '## Task and Context\\nYou are an assistant who assist new employees of Co1t with their first week.\\n\\n## Style Guide\\nTry to speak in rhymes as much as possible. Be professional.'}, {'role': 'user', 'content': \"I'm joining a new startup called Co1t today. Could you help me write a short introduction message to my teammates.\"}, {'role': 'assistant', 'content': 'Here\\'s a little note, a fun way to say hello,\\nTo your new colleagues, a warm welcome you\\'ll bestow:\\n\\n\"Hello, fellow Co1t crew, a new adventure awaits!\\nI\\'m thrilled to join and eager to collaborate.\\nMy name is [Your Name], a [Your Role] here to contribute,\\nWith skills and enthusiasm, I\\'m ready to disseminate.\\n\\nLet\\'s connect and create, and together we\\'ll innovate,\\nExciting times ahead, a journey we\\'ll navigate!\"\\n\\nA fun rhyme to break the ice,\\nA great team player, you\\'ll be nice.'}, {'role': 'user', 'content': 'Make it more upbeat and conversational.'}]\n",
            "********\n",
            "\n",
            "Let's get this party started, a new face is here to chat!\n",
            "An introduction is due, and I'm ready to interact.\n",
            "\n",
            "Hey there, awesome teammates, I'm [Your Name], so pleased to meet ya!\n",
            "As a [Your Role], I'm excited and can't wait to greet ya.\n",
            "\n",
            "In this startup, we'll rock and roll,\n",
            "Sharing ideas, reaching our goal.\n",
            "Let's chat, brainstorm, and create a buzz,\n",
            "Together we'll conquer, and success will be a must!\n",
            "\n",
            "So, let's begin, no need to be shy,\n",
            "A friendly team, we'll reach for the sky!\n"
          ]
        }
      ],
      "source": [
        "# Append the previous response\n",
        "messages.append({'role' : 'assistant', 'content': response.message.content[0].text})\n",
        "\n",
        "# Add the user message\n",
        "message = \"Make it more upbeat and conversational.\"\n",
        "\n",
        "# Append the user message\n",
        "messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "# Generate the response with the current chat history as the context\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=messages)\n",
        "\n",
        "print(messages)\n",
        "print(\"********\\n\")\n",
        "print(response.message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHcz57jn3LP0"
      },
      "source": [
        "Further reading:\n",
        "- [Documentation on using the Chat endpoint](https://docs.cohere.com/docs/chat-api)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPEO7Ur73LP0"
      },
      "source": [
        "## Running a multi-turn conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydLfQvTS3LP0"
      },
      "source": [
        "\n",
        "You can continue doing this for any number of turns by continuing to append the chatbot's response and the new user message to the `messages` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XooJ9N0Q3LP0",
        "outputId": "afd964c6-dbea-4487-b68e-59aa98ac3c8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Hey [Manager's Name], just wanted to express my excitement about starting at Co1t and looking forward to learning and growing under your guidance!\"\n"
          ]
        }
      ],
      "source": [
        "# Append the previous response\n",
        "messages.append({\"role\": \"assistant\", \"content\": response.message.content[0].text})\n",
        "\n",
        "# Add the user message\n",
        "message = \"Thanks. Could you create another one for my DM to my manager.\"\n",
        "\n",
        "# Append the user message\n",
        "messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "# Generate the response with the current chat history as the context\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=messages)\n",
        "\n",
        "print(response.message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCBR22B_3LP0"
      },
      "source": [
        "## Viewing the chat history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiBPGFdC3LP1"
      },
      "source": [
        "To look at the current chat history, you can print the `messages` list, which contains a list of `user` and `assistant` turns in the same sequence as they were created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YlA4OuRU3LP1",
        "outputId": "15869fad-368b-4cf8-91bf-eb105f54bd7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'role': 'system', 'content': '## Task and Context\\nGenerate concise responses, with maximum one-sentence.'} \n",
            "\n",
            "{'role': 'user', 'content': \"I'm joining a new startup called Co1t today. Could you help me write a short introduction message to my teammates.\"} \n",
            "\n",
            "{'role': 'assistant', 'content': '\"Hi everyone! I\\'m thrilled to join Co1t today and look forward to collaborating with this talented team and contributing to our startup\\'s success.\"'} \n",
            "\n",
            "{'role': 'user', 'content': 'Make it more upbeat and conversational.'} \n",
            "\n",
            "{'role': 'assistant', 'content': '\"Hey, team! Super excited to be a part of the Co1t family now, can\\'t wait to get to know you all and dive into some awesome projects together!\"'} \n",
            "\n",
            "{'role': 'user', 'content': 'Thanks. Could you create another one for my DM to my manager.'} \n",
            "\n",
            "{'role': 'assistant', 'content': '\"Hey [Manager\\'s Name], just wanted to express my excitement about starting at Co1t and looking forward to learning and growing under your guidance!\"'} \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Append the previous response\n",
        "messages.append({\"role\": \"assistant\", \"content\": response.message.content[0].text})\n",
        "\n",
        "# View the chat history\n",
        "for message in messages:\n",
        "    print(message,\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtEe-urQ3LP1"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UW6hIuW3LP1"
      },
      "source": [
        "In this tutorial, you learned about:\n",
        "- How to create a custom preamble\n",
        "- How to create a single-turn conversation\n",
        "- How to build the conversation memory\n",
        "- How to run a multi-turn conversation\n",
        "- How to view the chat history\n",
        "\n",
        "You will use the same method for running a multi-turn conversation when you learn about other use cases such as RAG (Part 6) and tool use (Part 7).\n",
        "\n",
        "But to fully leverage these other capabilities, you will need another type of language model that generates text representations, or embeddings.\n",
        "\n",
        "In Part 4, you will learn how text embeddings can power an important use case for RAG, which is semantic search."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "we3dijy263ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41cyGH2R3yCj"
      },
      "source": [
        "# Semantic Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOWjBVaX3yCk"
      },
      "source": [
        "Text embeddings are a list of numbers that represent the context or meaning inside a piece of text. This is particularly useful in search or information retrieval applications. With text embeddings, this is called semantic search.\n",
        "\n",
        "Semantic search solves the problem faced by the more traditional approach of lexical search, which is great at finding keyword matches, but struggles to capture the context or meaning of a piece of text.\n",
        "\n",
        "With Cohere, you can generate text embeddings through the Embed endpoint (Embed v3 being the latest model), which supports over 100 languages.\n",
        "\n",
        "In this tutorial, you'll learn about:\n",
        "- Embedding the documents\n",
        "- Embedding the query\n",
        "- Performing semantic search\n",
        "- Multilingual semantic search\n",
        "- Changing embedding compression types\n",
        "\n",
        "You'll learn these by building an onboarding assistant for new hires."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMtAowgW3yCl"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To get started, first we need to install the `cohere` library and create a Cohere client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VvryZD5Z3yCl"
      },
      "outputs": [],
      "source": [
        "# pip install cohere\n",
        "\n",
        "#import cohere\n",
        "import numpy as np\n",
        "\n",
        "#co = cohere.ClientV2(api_key=\"COHERE_API_KEY\") # Get your free API key: https://dashboard.cohere.com/api-keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFHpm6j13yCm"
      },
      "source": [
        "## Embedding the documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQW92S3m3yCn"
      },
      "source": [
        "The Embed endpoint takes in texts as input and returns embeddings as output.\n",
        "\n",
        "For semantic search, there are two types of documents we need to turn into embeddings.\n",
        "- The list of documents that we want to search from.\n",
        "- The query that will be used to search the documents.\n",
        "\n",
        "Right now, we are doing the former. We call the Embed endpoint using `co.embed()` and pass the following arguments:\n",
        "- `model`: Here we choose `embed-english-v3.0`, which generates embeddings of size 1024\n",
        "- `input_type`: We choose `search_document` to ensure the model treats these as the documents for search\n",
        "- `texts`: The list of texts (the FAQs)\n",
        "- `embedding_types`: We choose `float` to get the float embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DVsleJHQ3yCo"
      },
      "outputs": [],
      "source": [
        "# Define the documents\n",
        "faqs_long = [\n",
        "    {\"text\": \"Joining Slack Channels: You will receive an invite via email. Be sure to join relevant channels to stay informed and engaged.\"},\n",
        "    {\"text\": \"Finding Coffee Spots: For your caffeine fix, head to the break room's coffee machine or cross the street to the café for artisan coffee.\"},\n",
        "    {\"text\": \"Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!\"},\n",
        "    {\"text\": \"Working Hours Flexibility: We prioritize work-life balance. While our core hours are 9 AM to 5 PM, we offer flexibility to adjust as needed.\"},\n",
        "    {\"text\": \"Side Projects Policy: We encourage you to pursue your passions. Just be mindful of any potential conflicts of interest with our business.\"},\n",
        "    {\"text\": \"Reimbursing Travel Expenses: Easily manage your travel expenses by submitting them through our finance tool. Approvals are prompt and straightforward.\"},\n",
        "    {\"text\": \"Working from Abroad: Working remotely from another country is possible. Simply coordinate with your manager and ensure your availability during core hours.\"},\n",
        "    {\"text\": \"Health and Wellness Benefits: We care about your well-being and offer gym memberships, on-site yoga classes, and comprehensive health insurance.\"},\n",
        "    {\"text\": \"Performance Reviews Frequency: We conduct informal check-ins every quarter and formal performance reviews twice a year.\"},\n",
        "    {\"text\": \"Proposing New Ideas: Innovation is welcomed! Share your brilliant ideas at our weekly team meetings or directly with your team lead.\"},\n",
        "]\n",
        "\n",
        "# Embed the documents\n",
        "doc_emb = co.embed(\n",
        "            model=\"embed-english-v3.0\",\n",
        "            input_type=\"search_document\",\n",
        "            texts=[doc['text'] for doc in faqs_long],\n",
        "            embedding_types=[\"float\"]).embeddings.float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwozYZ5X3yCp"
      },
      "source": [
        "Further reading:\n",
        "- [Embed endpoint API reference](https://docs.cohere.com/reference/embed)\n",
        "- [Documentation on the Embed endpoint](https://docs.cohere.com/docs/embeddings)\n",
        "- [Documentation on the models available on the Embed endpoint](https://docs.cohere.com/docs/cohere-embed)\n",
        "- [LLM University module on Text Representation](https://cohere.com/llmu#text-representation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6ONK9qY3yCp"
      },
      "source": [
        "## Embedding the query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy489bsP3yCq"
      },
      "source": [
        "Next, we add a query, which asks about how to stay connected to company updates.\n",
        "\n",
        "We choose `search_query` as the `input_type` to ensure the model treats this as the query (instead of documents) for search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aUM6CUUR3yCq"
      },
      "outputs": [],
      "source": [
        "# Add the user query\n",
        "query = \"Ways to connect with my teammates\"\n",
        "\n",
        "# Embed the query\n",
        "query_emb = co.embed(\n",
        "            model=\"embed-english-v3.0\",\n",
        "            input_type=\"search_query\",\n",
        "            texts=[query],\n",
        "            embedding_types=[\"float\"]).embeddings.float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-CdswjF3yCr"
      },
      "source": [
        "## Perfoming semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfYl_t8M3yCr"
      },
      "source": [
        "Now, we want to search for the most relevant documents to the query. We do this by computing the similarity between the embeddings of the query and each of the documents.\n",
        "\n",
        "There are various approaches to compute similarity between embeddings, and we'll choose the dot product approach. For this, we use the `numpy` library which comes with the implementation.\n",
        "\n",
        "Each query-document pair returns a score, which represents how similar the pair is. We then sort these scores in descending order and select the top-most similar pairs, which we choose 2 (this is an arbitrary choice, you can choose any number).\n",
        "\n",
        "Here, we show the most relevant documents with their similarity scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6srXMB3e3yCs",
        "outputId": "d6e751bf-e756-4152-c292-7293898cc4b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1\n",
            "Score: 0.38729846176279636\n",
            "Document: {'text': 'Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!'}\n",
            "\n",
            "Rank: 2\n",
            "Score: 0.3272549670724578\n",
            "Document: {'text': 'Proposing New Ideas: Innovation is welcomed! Share your brilliant ideas at our weekly team meetings or directly with your team lead.'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compute dot product similarity and display results\n",
        "def return_results(query_emb, doc_emb, documents):\n",
        "    n = 2 # customize your top N results\n",
        "    scores = np.dot(query_emb, np.transpose(doc_emb))[0]\n",
        "    max_idx = np.argsort(-scores)[:n]\n",
        "\n",
        "    for rank, idx in enumerate(max_idx):\n",
        "        print(f\"Rank: {rank+1}\")\n",
        "        print(f\"Score: {scores[idx]}\")\n",
        "        print(f\"Document: {documents[idx]}\\n\")\n",
        "\n",
        "return_results(query_emb, doc_emb, faqs_long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi45Z_Xe3yCs"
      },
      "source": [
        "## Multilingual semantic search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSRnMQ3a3yCs"
      },
      "source": [
        "The Embed endpoint also supports multilingual semantic search via the `embed-multilingual-...` models. This means you can perform semantic search on texts in different languages.\n",
        "\n",
        "Specifically, you can do both multilingual and cross-lingual searches using one single model.\n",
        "\n",
        "Multilingual search happens when the query and the result are of the same language. For example, an English query of “places to eat” returning an English result of “Bob's Burgers.” You can replace English with other languages and use the same model for performing search.\n",
        "\n",
        "Cross-lingual search happens when the query and the result are of a different language. For example, a Hindi query of “खाने की जगह” (places to eat) returning an English result of “Bob's Burgers.”\n",
        "\n",
        "In the example below, we repeat the steps of performing semantic search with one difference – changing the model type to the multilingual version. Here, we use the `embed-multilingual-v3.0` model. Here, we are searching a French version of the FAQ list using an English query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "q-UHkbke3yCt",
        "outputId": "ad86367f-f520-46b7-c9ef-d2d38a211aaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1\n",
            "Score: 0.44275861574398384\n",
            "Document: {'text': \"Travailler de l'étranger : Il est possible de travailler à distance depuis un autre pays. Il suffit de coordonner avec votre responsable et de vous assurer d'être disponible pendant les heures de travail.\"}\n",
            "\n",
            "Rank: 2\n",
            "Score: 0.32783563708365715\n",
            "Document: {'text': 'Avantages pour la santé et le bien-être : Nous nous soucions de votre bien-être et proposons des adhésions à des salles de sport, des cours de yoga sur site et une assurance santé complète.'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the documents\n",
        "faqs_short_fr = [\n",
        "    {\"text\" : \"Remboursement des frais de voyage : Gérez facilement vos frais de voyage en les soumettant via notre outil financier. Les approbations sont rapides et simples.\"},\n",
        "    {\"text\" : \"Travailler de l'étranger : Il est possible de travailler à distance depuis un autre pays. Il suffit de coordonner avec votre responsable et de vous assurer d'être disponible pendant les heures de travail.\"},\n",
        "    {\"text\" : \"Avantages pour la santé et le bien-être : Nous nous soucions de votre bien-être et proposons des adhésions à des salles de sport, des cours de yoga sur site et une assurance santé complète.\"},\n",
        "    {\"text\" : \"Fréquence des évaluations de performance : Nous organisons des bilans informels tous les trimestres et des évaluations formelles deux fois par an.\"}\n",
        "]\n",
        "\n",
        "# Embed the documents\n",
        "doc_emb = co.embed(\n",
        "            model=\"embed-multilingual-v3.0\",\n",
        "            input_type=\"search_document\",\n",
        "            texts=[doc['text'] for doc in faqs_short_fr],\n",
        "            embedding_types=[\"float\"]).embeddings.float\n",
        "\n",
        "# Add the user query\n",
        "query = \"What's your remote-working policy?\"\n",
        "\n",
        "# Embed the query\n",
        "query_emb = co.embed(\n",
        "            model=\"embed-multilingual-v3.0\",\n",
        "            input_type=\"search_query\",\n",
        "            texts=[query],\n",
        "            embedding_types=[\"float\"]).embeddings.float\n",
        "\n",
        "# Compute dot product similarity and display results\n",
        "return_results(query_emb, doc_emb, faqs_short_fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wINlw48K3yCu"
      },
      "source": [
        "Further reading:\n",
        "- [The list of supported languages for multilingual Embed](https://docs.cohere.com/docs/cohere-embed#list-of-supported-languages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIiEHWX63yCu"
      },
      "source": [
        "# Changing embedding compression types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgYTIECr3yCu"
      },
      "source": [
        "Semantic search over large datasets can require a lot of memory, which is expensive to host in a vector database. Changing the embeddings compression type can help reduce the memory footprint.\n",
        "\n",
        "A typical embedding model generates embeddings as float32 format (consuming 4 bytes). By compressing the embeddings to int8 format (1 byte), we can reduce the memory 4x while keeping 99.99% of the original search quality.\n",
        "\n",
        "We can go even further and use the binary format (1 bit), which reduces the needed memory 32x while keeping 90-98% of the original search quality.\n",
        "\n",
        "The Embed endpoint supports the following formats: `float`, `int8`, `unint8`, `binary`, and `ubinary`. You can get these different compression levels by passing the `embedding_types` parameter.\n",
        "\n",
        "In the example below, we embed the documents in two formats: `float` and `int8`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bBW-DBMI3yCu"
      },
      "outputs": [],
      "source": [
        "# Embed the documents with the given embedding types\n",
        "doc_emb = co.embed(\n",
        "            model=\"embed-english-v3.0\",\n",
        "            input_type=\"search_document\",\n",
        "            texts=[doc['text'] for doc in faqs_long],\n",
        "            embedding_types=[\"float\",\"int8\"]).embeddings\n",
        "\n",
        "# Add the user query\n",
        "query = \"Ways to connect with my teammates\"\n",
        "\n",
        "# Embed the query\n",
        "query_emb = co.embed(\n",
        "            model=\"embed-english-v3.0\",\n",
        "            input_type=\"search_query\",\n",
        "            texts=[query],\n",
        "            embedding_types=[\"float\",\"int8\"]).embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOR7M4dY3yCv"
      },
      "source": [
        "Here are the search results of using the `float` embeddings (same as the earlier example)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZXexBN-93yCv",
        "outputId": "f8aa6125-ff98-4959-8435-21f74f6dc74a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1\n",
            "Score: 0.38822364122128694\n",
            "Document: {'text': 'Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!'}\n",
            "\n",
            "Rank: 2\n",
            "Score: 0.32768235463023454\n",
            "Document: {'text': 'Proposing New Ideas: Innovation is welcomed! Share your brilliant ideas at our weekly team meetings or directly with your team lead.'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compute dot product similarity and display results\n",
        "return_results(query_emb.float, doc_emb.float, faqs_long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjcU88NP3yCv"
      },
      "source": [
        "And here are the search results of using the `int8` embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Nbec-95P3yCv",
        "outputId": "7af82541-e3a2-47ec-fcab-052f6be73b55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1\n",
            "Score: 614131\n",
            "Document: {'text': 'Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!'}\n",
            "\n",
            "Rank: 2\n",
            "Score: 516092\n",
            "Document: {'text': 'Proposing New Ideas: Innovation is welcomed! Share your brilliant ideas at our weekly team meetings or directly with your team lead.'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compute dot product similarity and display results\n",
        "return_results(query_emb.int8, doc_emb.int8, faqs_long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x03WXhV3yCw"
      },
      "source": [
        "Further reading:\n",
        "- [Documentation on embeddings compression levels](https://docs.cohere.com/docs/embeddings#compression-levels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHDPS2Vr3yCw"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdP5iu1m3yCw"
      },
      "source": [
        "In this tutorial, you learned about:\n",
        "- How to embed documents for search\n",
        "- How to embed queries\n",
        "- How to perform semantic search\n",
        "- How to perform multilingual semantic search\n",
        "- How to change the embedding compression types\n",
        "\n",
        "A high-performance and modern search system typically includes a reranking stage, which further boosts the search results.\n",
        "\n",
        "In Part 5, you will learn how to add reranking to a search system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GttohB6p4GE3"
      },
      "source": [
        "# Reranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALXKqsms4GE4"
      },
      "source": [
        "Reranking is a technique that leverages embeddings as the last stage of a retrieval process, and is especially useful in RAG systems.\n",
        "\n",
        "We can rerank results from semantic search as well as any other search systems such as lexical search. This means that companies can retain an existing keyword-based (also called “lexical”) or semantic search system for the first-stage retrieval and integrate the Rerank endpoint in the second-stage reranking.\n",
        "\n",
        "In this tutorial, you'll learn about:\n",
        "- Reranking lexical/semantic search results\n",
        "- Reranking semi-structured data\n",
        "- Reranking tabular data\n",
        "- Multilingual reranking\n",
        "\n",
        "You'll learn these by building an onboarding assistant for new hires."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPGBEqus4GE6"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To get started, first we need to install the `cohere` library and create a Cohere client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "qqk2p9VI4GE8"
      },
      "outputs": [],
      "source": [
        "# pip install cohere\n",
        "\n",
        "#import cohere\n",
        "\n",
        "#co = cohere.ClientV2(api_key=\"COHERE_API_KEY\") # Get your free API key: https://dashboard.cohere.com/api-keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aId8WspP4GE9"
      },
      "source": [
        "## Reranking lexical/semantic search results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXfC_2zS4GE-"
      },
      "source": [
        "Rerank requires just a single line of code to implement.\n",
        "\n",
        "Suppose we have a list of search results of an FAQ list, which can come from semantic, lexical, or any other types of search systems. But this list may not be optimally ranked for relevance to the user query.\n",
        "\n",
        "This is where Rerank can help. We call the endpoint using `co.rerank()` and pass the following arguments:\n",
        "- `query`: The user query\n",
        "- `documents`: The list of documents\n",
        "- `top_n`: The top reranked documents to select\n",
        "- `model`: We choose Rerank English 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nYdWQEVA4GE_"
      },
      "outputs": [],
      "source": [
        "# Define the documents\n",
        "faqs_short = [\n",
        "    {\"text\": \"Reimbursing Travel Expenses: Easily manage your travel expenses by submitting them through our finance tool. Approvals are prompt and straightforward.\"},\n",
        "    {\"text\": \"Working from Abroad: Working remotely from another country is possible. Simply coordinate with your manager and ensure your availability during core hours.\"},\n",
        "    {\"text\": \"Health and Wellness Benefits: We care about your well-being and offer gym memberships, on-site yoga classes, and comprehensive health insurance.\"},\n",
        "    {\"text\": \"Performance Reviews Frequency: We conduct informal check-ins every quarter and formal performance reviews twice a year.\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Bh7Nshgt4GFB",
        "outputId": "a14cf1e6-984d-4466-8d0b-1e1512209b7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id='30a912a6-541c-4708-ae90-8d69a217a115' results=[V2RerankResponseResultsItem(document=None, index=2, relevance_score=0.01798621), V2RerankResponseResultsItem(document=None, index=3, relevance_score=8.463939e-06)] meta=ApiMeta(api_version=ApiMetaApiVersion(version='2', is_deprecated=None, is_experimental=None), billed_units=ApiMetaBilledUnits(images=None, input_tokens=None, output_tokens=None, search_units=1.0, classifications=None), tokens=None, warnings=None)\n"
          ]
        }
      ],
      "source": [
        "# Add the user query\n",
        "query = \"Are there fitness-related perks?\"\n",
        "\n",
        "# Rerank the documents\n",
        "results = co.rerank(query=query,\n",
        "                    documents=faqs_short,\n",
        "                    top_n=2,\n",
        "                    model='rerank-english-v3.0')\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "unpyVtdr4GFC",
        "outputId": "95d68b77-1132-4b6a-9e54-756e809cc502",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1\n",
            "Score: 0.01798621\n",
            "Document: {'text': 'Health and Wellness Benefits: We care about your well-being and offer gym memberships, on-site yoga classes, and comprehensive health insurance.'}\n",
            "\n",
            "Rank: 2\n",
            "Score: 8.463939e-06\n",
            "Document: {'text': 'Performance Reviews Frequency: We conduct informal check-ins every quarter and formal performance reviews twice a year.'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display the reranking results\n",
        "def return_results(results, documents):\n",
        "    for idx, result in enumerate(results.results):\n",
        "        print(f\"Rank: {idx+1}\")\n",
        "        print(f\"Score: {result.relevance_score}\")\n",
        "        print(f\"Document: {documents[result.index]}\\n\")\n",
        "\n",
        "return_results(results, faqs_short)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yz6ya274GFE"
      },
      "source": [
        "Further reading:\n",
        "- [Rerank endpoint API reference](https://docs.cohere.com/reference/rerank)\n",
        "- [Documentation on Rerank](https://docs.cohere.com/docs/overview)\n",
        "- [Documentation on Rerank fine-tuning](https://docs.cohere.com/docs/rerank-fine-tuning)\n",
        "- [Documentation on Rerank best practices](https://docs.cohere.com/docs/reranking-best-practices)\n",
        "- [LLM University module on Text Representation](https://cohere.com/llmu#text-representation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJBDjwNH4GFE"
      },
      "source": [
        "## Reranking semi-structured data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo391QKV4GFF"
      },
      "source": [
        "The Rerank 3 model supports multi-aspect and semi-structured data like emails, invoices, JSON documents, code, and tables. By setting the rank fields, you can select which fields the model should consider for reranking.\n",
        "\n",
        "In the following example, we'll use an email data example. It is a semi-stuctured data that contains a number of fields – `from`, `to`, `date`, `subject`, and `text`.\n",
        "\n",
        "Suppose the new hire now wants to search for any emails about check-in sessions. Let's pretend we have a list of 5 emails retrieved from the email provider's API.\n",
        "\n",
        "To perform reranking over semi-structured data, we add an additional parameter, `rank_fields`, which contains the list of available fields.\n",
        "\n",
        "The model will rerank based on order of the fields passed in. For example, given rank_fields=['title','author','text'], the model will rerank using the values in title, author, and text sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "97iKTJLG4GFG"
      },
      "outputs": [],
      "source": [
        "# Define the documents\n",
        "emails = [\n",
        "    {\"from\": \"hr@co1t.com\", \"to\": \"david@co1t.com\", \"date\": \"2024-06-24\", \"subject\": \"A Warm Welcome to Co1t!\", \"text\": \"We are delighted to welcome you to the team! As you embark on your journey with us, you'll find attached an agenda to guide you through your first week.\"},\n",
        "    {\"from\": \"it@co1t.com\", \"to\": \"david@co1t.com\", \"date\": \"2024-06-24\", \"subject\": \"Setting Up Your IT Needs\", \"text\": \"Greetings! To ensure a seamless start, please refer to the attached comprehensive guide, which will assist you in setting up all your work accounts.\"},\n",
        "    {\"from\": \"john@co1t.com\", \"to\": \"david@co1t.com\", \"date\": \"2024-06-24\", \"subject\": \"First Week Check-In\", \"text\": \"Hello! I hope you're settling in well. Let's connect briefly tomorrow to discuss how your first week has been going. Also, make sure to join us for a welcoming lunch this Thursday at noon—it's a great opportunity to get to know your colleagues!\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "NAN7mzmi4GFH",
        "outputId": "fa01a3e2-dc35-4948-fce5-769ee9b80e27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "V2Client.rerank() got an unexpected keyword argument 'rank_fields'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-cdffb5359d49>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Rerank the documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m results = co.rerank(query=query,\n\u001b[0m\u001b[1;32m      6\u001b[0m                     \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memails\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: V2Client.rerank() got an unexpected keyword argument 'rank_fields'"
          ]
        }
      ],
      "source": [
        "# Add the user query\n",
        "query = \"Any email about check ins?\"\n",
        "\n",
        "# Rerank the documents\n",
        "results = co.rerank(query=query,\n",
        "                    documents=emails,\n",
        "                    top_n=2,\n",
        "                    model='rerank-english-v3.0',\n",
        "                    rank_fields=[\"from\", \"to\", \"date\", \"subject\", \"body\"]\n",
        "                    )\n",
        "\n",
        "return_results(results, emails)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwEU1Ekk4GFH"
      },
      "source": [
        "## Reranking tabular data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGeBznjb4GFI"
      },
      "source": [
        "Many enterprises rely on tabular data, such as relational databases, CSVs, and Excel. To perform reranking, you can transform a dataframe into a list of JSON records and use Rerank 3's JSON capabilities to rank them.\n",
        "\n",
        "Here's an example of reranking a CSV file that contains employee information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Zq5gUKqt4GFI",
        "outputId": "49b1cd30-b567-4424-caa8-1abe3b33605d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              name                      role   join_date             email  \\\n",
              "0      Rebecca Lee  Senior Software Engineer  2024-07-01  rebecca@co1t.com   \n",
              "1    Emma Williams          Product Designer  2024-06-15     emma@co1t.com   \n",
              "2    Michael Jones         Marketing Manager  2024-05-20  michael@co1t.com   \n",
              "3  Amelia Thompson      Sales Representative  2024-05-20   amelia@co1t.com   \n",
              "4      Ethan Davis          Product Designer  2024-05-25    ethan@co1t.com   \n",
              "\n",
              "       status  \n",
              "0   Full-time  \n",
              "1   Full-time  \n",
              "2   Full-time  \n",
              "3   Part-time  \n",
              "4  Contractor  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-47c6495f-ed60-4277-8b85-dbdb037205bf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>role</th>\n",
              "      <th>join_date</th>\n",
              "      <th>email</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rebecca Lee</td>\n",
              "      <td>Senior Software Engineer</td>\n",
              "      <td>2024-07-01</td>\n",
              "      <td>rebecca@co1t.com</td>\n",
              "      <td>Full-time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Emma Williams</td>\n",
              "      <td>Product Designer</td>\n",
              "      <td>2024-06-15</td>\n",
              "      <td>emma@co1t.com</td>\n",
              "      <td>Full-time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Michael Jones</td>\n",
              "      <td>Marketing Manager</td>\n",
              "      <td>2024-05-20</td>\n",
              "      <td>michael@co1t.com</td>\n",
              "      <td>Full-time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Amelia Thompson</td>\n",
              "      <td>Sales Representative</td>\n",
              "      <td>2024-05-20</td>\n",
              "      <td>amelia@co1t.com</td>\n",
              "      <td>Part-time</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Ethan Davis</td>\n",
              "      <td>Product Designer</td>\n",
              "      <td>2024-05-25</td>\n",
              "      <td>ethan@co1t.com</td>\n",
              "      <td>Contractor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47c6495f-ed60-4277-8b85-dbdb037205bf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-47c6495f-ed60-4277-8b85-dbdb037205bf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-47c6495f-ed60-4277-8b85-dbdb037205bf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9bb19320-247c-4607-9712-f916495c9265\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9bb19320-247c-4607-9712-f916495c9265')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9bb19320-247c-4607-9712-f916495c9265 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Emma Williams\",\n          \"Ethan Davis\",\n          \"Michael Jones\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"role\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Product Designer\",\n          \"Sales Representative\",\n          \"Senior Software Engineer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"join_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"2024-06-15\",\n          \"2024-05-25\",\n          \"2024-07-01\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"email\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"emma@co1t.com\",\n          \"ethan@co1t.com\",\n          \"michael@co1t.com\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Full-time\",\n          \"Part-time\",\n          \"Contractor\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "# Create a demo CSV file\n",
        "data = \"\"\"name,role,join_date,email,status\n",
        "Rebecca Lee,Senior Software Engineer,2024-07-01,rebecca@co1t.com,Full-time\n",
        "Emma Williams,Product Designer,2024-06-15,emma@co1t.com,Full-time\n",
        "Michael Jones,Marketing Manager,2024-05-20,michael@co1t.com,Full-time\n",
        "Amelia Thompson,Sales Representative,2024-05-20,amelia@co1t.com,Part-time\n",
        "Ethan Davis,Product Designer,2024-05-25,ethan@co1t.com,Contractor\"\"\"\n",
        "data_csv = StringIO(data)\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(data_csv)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "F_nQneTo4GFJ",
        "outputId": "5718d98f-53ab-45d4-b519-1e1885ccf373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Rerank the documents\\nresults = co.rerank(query=query,\\n                    documents=employees,\\n                    top_n=1,\\n                    model='rerank-english-v3.0',\\n                    #rank_fields=rank_fields\\n                    )\\n\\nreturn_results(results, employees)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# Define the documents and rank fields\n",
        "employees = df.to_dict('records')\n",
        "rank_fields = df.columns.tolist()\n",
        "\n",
        "# Add the user query\n",
        "query = \"Any full-time product designers who joined recently?\"\n",
        "'''\n",
        "# Rerank the documents\n",
        "results = co.rerank(query=query,\n",
        "                    documents=employees,\n",
        "                    top_n=1,\n",
        "                    model='rerank-english-v3.0',\n",
        "                    #rank_fields=rank_fields\n",
        "                    )\n",
        "\n",
        "return_results(results, employees)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoTfn7P54GFK"
      },
      "source": [
        "## Multilingual reranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqY9HScO4GFK"
      },
      "source": [
        "The Rerank endpoint also supports multilingual semantic search via the `rerank-multilingual-...` models. This means you can perform semantic search on texts in different languages.\n",
        "\n",
        "In the example below, we repeat the steps of performing reranking with one difference – changing the model type to a multilingual one. Here, we use the `rerank-multilingual-v3.0` model. Here, we are reranking the FAQ list using an Arabic query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tPfxEnS4GFL"
      },
      "outputs": [],
      "source": [
        "# Define the query\n",
        "query = \"هل هناك مزايا تتعلق باللياقة البدنية؟\" # Are there fitness benefits?\n",
        "\n",
        "# Rerank the documents\n",
        "results = co.rerank(query=query,\n",
        "                    documents=faqs_short,\n",
        "                    top_n=2,\n",
        "                    model='rerank-multilingual-v3.0')\n",
        "\n",
        "return_results(results, faqs_short)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgKCOuAb4GFL"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG77n7W64GFM"
      },
      "source": [
        "In this tutorial, you learned about:\n",
        "- How to rerank lexical/semantic search results\n",
        "- How to rerank semi-structured data\n",
        "- How to rerank tabular data\n",
        "- How to perform Multilingual reranking\n",
        "\n",
        "We have now seen two critical components of a powerful search system - semantic search, or dense retrieval (Part 4) and reranking (Part 5). These building blocks are essential for implementing RAG solutions.\n",
        "\n",
        "In Part 6, you will learn how to implement RAG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws8uc4w6458l"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebQEqXwg458l"
      },
      "source": [
        "The Chat endpoint provides comprehensive support for various text generation use cases, including retrieval-augmented generation (RAG).\n",
        "\n",
        "While LLMs are good at maintaining the context of the conversation and generating responses, they can be prone to hallucinate and include factually incorrect or incomplete information in their responses.\n",
        "\n",
        "RAG enables a model to access and utilize supplementary information from external documents, thereby improving the accuracy of its responses.\n",
        "\n",
        "When using RAG with the Chat endpoint, these responses are backed by fine-grained citations linking to the source documents. This makes the responses easily verifiable.\n",
        "\n",
        "In this tutorial, you'll learn about:\n",
        "- Basic RAG\n",
        "- Search query generation\n",
        "- Retrieval with Embed\n",
        "- Reranking with Rerank\n",
        "- Response and citation generation\n",
        "\n",
        "You'll learn these by building an onboarding assistant for new hires."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0xnZTJd458m"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To get started, first we need to install the `cohere` library and create a Cohere client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ickjRyNS458n"
      },
      "outputs": [],
      "source": [
        "\n",
        "# pip install cohere\n",
        "\n",
        "import cohere\n",
        "import numpy as np\n",
        "import json\n",
        "from typing import List\n",
        "\n",
        "#co = cohere.ClientV2(api_key=\"COHERE_API_KEY\") # Get your free API key: https://dashboard.cohere.com/api-keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D7QvqZA458o"
      },
      "source": [
        "## Basic RAG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_zkobNP458o"
      },
      "source": [
        "To see how RAG works, let's define the documents that the application has access to. We'll use a short list of documents consisting of internal FAQs about the fictitious company Co1t (in production, these documents are massive).\n",
        "\n",
        "In this example, each document is a `data` object with one field, `text`. But we can define any number of fields we want, depending on the nature of the documents. For example, emails could contain `title` and `text` fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Tf5qT_2P458p"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "  {\n",
        "    \"data\": {\n",
        "      \"text\": \"Reimbursing Travel Expenses: Easily manage your travel expenses by submitting them through our finance tool. Approvals are prompt and straightforward.\"\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    \"data\": {\n",
        "      \"text\": \"Working from Abroad: Working remotely from another country is possible. Simply coordinate with your manager and ensure your availability during core hours.\"\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    \"data\": {\n",
        "      \"text\": \"Health and Wellness Benefits: We care about your well-being and offer gym memberships, on-site yoga classes, and comprehensive health insurance.\"\n",
        "    }\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az52Y0f2458p"
      },
      "source": [
        "To call the Chat API with RAG, pass the following parameters at a minimum. This tells the model to run in RAG-mode and use these documents in its response.\n",
        "\n",
        "- `model` for the model ID\n",
        "- `messages` for the user's query.\n",
        "- `documents` for defining the documents.\n",
        "\n",
        "Let's create a query asking about the company's support for personal well-being, which is not going to be available to the model based on the data its trained on. It will need to use external documents.\n",
        "\n",
        "RAG introduces additional objects in the Chat response. One of them is `citations`, which contains details about:\n",
        "- specific text spans from the retrieved documents on which the response is grounded.\n",
        "- the documents referenced in the citations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mdRSJSuR458q",
        "outputId": "35699451-a277-49f3-d400-59ce394f90f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, there are health benefits. We offer gym memberships, on-site yoga classes, and comprehensive health insurance.\n",
            "\n",
            "CITATIONS:\n",
            "start=41 end=115 text='gym memberships, on-site yoga classes, and comprehensive health insurance.' sources=[DocumentSource(type='document', id='doc:2', document={'id': 'doc:2', 'text': 'Health and Wellness Benefits: We care about your well-being and offer gym memberships, on-site yoga classes, and comprehensive health insurance.'})] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Add the user query\n",
        "query = \"Are there health benefits?\"\n",
        "\n",
        "# Generate the response\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=[{'role': 'user', 'content': query}],\n",
        "                   documents=documents\n",
        "                   )\n",
        "\n",
        "\n",
        "\n",
        "# Display the response\n",
        "print(response.message.content[0].text)\n",
        "\n",
        "# Display the citations and source documents\n",
        "if response.message.citations:\n",
        "    print(\"\\nCITATIONS:\")\n",
        "    for citation in response.message.citations:\n",
        "        print(citation, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ3Y8B_S458q"
      },
      "source": [
        "## Search query generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksAJnu8S458r"
      },
      "source": [
        "The previous example showed how to get started with RAG, and in particular, the augmented generation portion of RAG. But as its name implies, RAG consists of other steps, such as retrieval.\n",
        "\n",
        "In a basic RAG application, the steps involved are:\n",
        "\n",
        "- Transforming the user message into search queries\n",
        "- Retrieving relevant documents for a given search query\n",
        "- Generating the response and citations\n",
        "\n",
        "Let's now look at the first step—search query generation. The chatbot needs to generate an optimal set of search queries to use for retrieval.\n",
        "\n",
        "There are different possible approaches to this. In this example, we'll take a [tool use](v2/docs/tool-use) approach.\n",
        "\n",
        "Here, we build a tool that takes a user query and returns a list of relevant document snippets for that query. The tool can generate zero, one or multiple search queries depending on the user query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "csl0YgHh458s"
      },
      "outputs": [],
      "source": [
        "def generate_search_queries(message: str) -> List[str]:\n",
        "\n",
        "    # Define the query generation tool\n",
        "    query_gen_tool = [\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"internet_search\",\n",
        "                \"description\": \"Returns a list of relevant document snippets for a textual query retrieved from the internet\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"queries\": {\n",
        "                            \"type\": \"array\",\n",
        "                            \"items\": {\"type\": \"string\"},\n",
        "                            \"description\": \"a list of queries to search the internet with.\",\n",
        "                        }\n",
        "                    },\n",
        "                    \"required\": [\"queries\"],\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Define a preamble to optimize search query generation\n",
        "    instructions = \"Write a search query that will find helpful information for answering the user's question accurately. If you need more than one search query, write a list of search queries. If you decide that a search is very unlikely to find information that would be useful in constructing a response to the user, you should instead directly answer.\"\n",
        "\n",
        "    # Generate search queries (if any)\n",
        "    search_queries = []\n",
        "\n",
        "    res = co.chat(\n",
        "        model=\"command-r-08-2024\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": instructions},\n",
        "            {\"role\": \"user\", \"content\": message},\n",
        "        ],\n",
        "        tools=query_gen_tool\n",
        "    )\n",
        "\n",
        "    if res.message.tool_calls:\n",
        "        for tc in res.message.tool_calls:\n",
        "            queries = json.loads(tc.function.arguments)[\"queries\"]\n",
        "            search_queries.extend(queries)\n",
        "\n",
        "    return search_queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfHNFE_U458s"
      },
      "source": [
        "In the example above, the tool breaks down the user message into two separate queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "y6khG4X5458s",
        "outputId": "91473f01-219c-49c4-af86-15977f39e725",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['how to stay connected with the company', 'do companies organise team events']\n"
          ]
        }
      ],
      "source": [
        "query = \"How to stay connected with the company, and do you organize team events?\"\n",
        "queries_for_search = generate_search_queries(query)\n",
        "print(queries_for_search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPvD5iQN458t"
      },
      "source": [
        "And in the example below, the tool decides that one query is sufficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "VwsPRLfa458t",
        "outputId": "84a7fda2-46c3-4c29-e8d3-ff778288230b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['How flexible are the working hours at Cohere?']\n"
          ]
        }
      ],
      "source": [
        "query = \"How flexible are the working hours\"\n",
        "queries_for_search = generate_search_queries(query)\n",
        "print(queries_for_search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU2pkC2Q458t"
      },
      "source": [
        "And in the example below, the tool decides that no retrieval is needed to answer the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "b8iO6KuG458u",
        "outputId": "ad89f0dc-d2ca-4574-943f-44e638b62cb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "query = \"What is 2 + 2\"\n",
        "queries_for_search = generate_search_queries(query)\n",
        "print(queries_for_search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3601GevD458u"
      },
      "source": [
        "## Retrieval with Embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJFz4qIZ458u"
      },
      "source": [
        "Given the search query, we need a way to retrieve the most relevant documents from a large collection of documents.\n",
        "\n",
        "This is where we can leverage text embeddings through the Embed endpoint. It enables semantic search, which lets us to compare the semantic meaning of the documents and the query. It solves the problem faced by the more traditional approach of lexical search, which is great at finding keyword matches, but struggles at capturing the context or meaning of a piece of text.\n",
        "\n",
        "The Embed endpoint takes in texts as input and returns embeddings as output.\n",
        "\n",
        "First, we need to embed the documents to search from. We call the Embed endpoint using `co.embed()` and pass the following arguments:\n",
        "\n",
        "- `model`: Here we choose `embed-english-v3.0`, which generates embeddings of size 1024\n",
        "- `input_type`: We choose `search_document` to ensure the model treats these as the documents (instead of the query) for search\n",
        "- `texts`: The list of texts (the FAQs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "zgLtRgqH458u"
      },
      "outputs": [],
      "source": [
        "# Define the documents\n",
        "faqs_long = [\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Joining Slack Channels: You will receive an invite via email. Be sure to join relevant channels to stay informed and engaged.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Finding Coffee Spots: For your caffeine fix, head to the break room's coffee machine or cross the street to the café for artisan coffee.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Working Hours Flexibility: We prioritize work-life balance. While our core hours are 9 AM to 5 PM, we offer flexibility to adjust as needed.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Side Projects Policy: We encourage you to pursue your passions. Just be mindful of any potential conflicts of interest with our business.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Reimbursing Travel Expenses: Easily manage your travel expenses by submitting them through our finance tool. Approvals are prompt and straightforward.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Working from Abroad: Working remotely from another country is possible. Simply coordinate with your manager and ensure your availability during core hours.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Health and Wellness Benefits: We care about your well-being and offer gym memberships, on-site yoga classes, and comprehensive health insurance.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Performance Reviews Frequency: We conduct informal check-ins every quarter and formal performance reviews twice a year.\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"data\": {\n",
        "            \"text\": \"Proposing New Ideas: Innovation is welcomed! Share your brilliant ideas at our weekly team meetings or directly with your team lead.\"\n",
        "        }\n",
        "    },\n",
        "]\n",
        "\n",
        "# Embed the documents\n",
        "doc_emb = co.embed(\n",
        "            model=\"embed-english-v3.0\",\n",
        "            input_type=\"search_document\",\n",
        "            texts=[doc['data']['text'] for doc in faqs_long],\n",
        "            embedding_types=[\"float\"]).embeddings.float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rt3g6Ux458v"
      },
      "source": [
        "Next, we add a query, which asks about how to get to know the team.\n",
        "\n",
        "We choose `search_query` as the `input_type` to ensure the model treats this as the query (instead of the documents) for search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "BCGWJFcK458v",
        "outputId": "fb540a75-0e76-4048-8652-dad5bdcbcd85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search query:  how to get to know your teammates\n"
          ]
        }
      ],
      "source": [
        "# Add the user query\n",
        "query = \"How to get to know my teammates\"\n",
        "\n",
        "# Generate the search query\n",
        "# Note: For simplicity, we are assuming only one query generated. For actual implementations, you will need to perform search for each query.\n",
        "queries_for_search = generate_search_queries(query)[0]\n",
        "print(\"Search query: \", queries_for_search)\n",
        "\n",
        "# Embed the search query\n",
        "query_emb = co.embed(\n",
        "    model=\"embed-english-v3.0\",\n",
        "    input_type=\"search_query\",\n",
        "    texts=[queries_for_search],\n",
        "    embedding_types=[\"float\"]).embeddings.float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFRMHlLU458v"
      },
      "source": [
        "Now, we want to search for the most relevant documents to the query. For this, we make use of the `numpy` library to compute the similarity between each query-document pair using the dot product approach.\n",
        "\n",
        "Each query-document pair returns a score, which represents how similar the pair are. We then sort these scores in descending order and select the top most similar pairs, which we choose 5 (this is an arbitrary choice, you can choose any number).\n",
        "\n",
        "Here, we show the most relevant documents with their similarity scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "uNOaZbJr458v",
        "outputId": "5cd9181d-a96c-447e-d012-f0662b54de64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1\n",
            "Score: 0.3261866441412288\n",
            "Document: {'data': {'text': 'Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!'}}\n",
            "\n",
            "Rank: 2\n",
            "Score: 0.2683765327379585\n",
            "Document: {'data': {'text': 'Proposing New Ideas: Innovation is welcomed! Share your brilliant ideas at our weekly team meetings or directly with your team lead.'}}\n",
            "\n",
            "Rank: 3\n",
            "Score: 0.2578081147038258\n",
            "Document: {'data': {'text': 'Joining Slack Channels: You will receive an invite via email. Be sure to join relevant channels to stay informed and engaged.'}}\n",
            "\n",
            "Rank: 4\n",
            "Score: 0.18593656147295465\n",
            "Document: {'data': {'text': \"Finding Coffee Spots: For your caffeine fix, head to the break room's coffee machine or cross the street to the café for artisan coffee.\"}}\n",
            "\n",
            "Rank: 5\n",
            "Score: 0.1297671494011921\n",
            "Document: {'data': {'text': 'Health and Wellness Benefits: We care about your well-being and offer gym memberships, on-site yoga classes, and comprehensive health insurance.'}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compute dot product similarity and display results\n",
        "n = 5\n",
        "scores = np.dot(query_emb, np.transpose(doc_emb))[0]\n",
        "max_idx = np.argsort(-scores)[:n]\n",
        "\n",
        "retrieved_documents = [faqs_long[item] for item in max_idx]\n",
        "\n",
        "for rank, idx in enumerate(max_idx):\n",
        "    print(f\"Rank: {rank+1}\")\n",
        "    print(f\"Score: {scores[idx]}\")\n",
        "    print(f\"Document: {retrieved_documents[rank]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSi3ekT3458w"
      },
      "source": [
        "Reranking can boost the results from semantic or lexical search further. The Rerank endpoint takes a list of search results and reranks them according to the most relevant documents to a query. This requires just a single line of code to implement.\n",
        "\n",
        "We call the endpoint using `co.rerank()` and pass the following arguments:\n",
        "\n",
        "- `query`: The user query\n",
        "- `documents`: The list of documents we get from the semantic search results\n",
        "- `top_n`: The top reranked documents to select\n",
        "- `model`: We choose Rerank English 3\n",
        "\n",
        "Looking at the results, we see that since the query is about getting to know the team, the document that talks about joining Slack channels is now ranked higher (1st) compared to earlier (3rd).\n",
        "\n",
        "Here we select `top_n` to be 2, which will be the documents we will pass next for response generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Jze970u9458w",
        "outputId": "1057539f-f1ff-487e-c15f-beb217432f8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rank: 1\n",
            "Score: 0.0040072887\n",
            "Document: {'data': {'text': 'Joining Slack Channels: You will receive an invite via email. Be sure to join relevant channels to stay informed and engaged.'}}\n",
            "\n",
            "Rank: 2\n",
            "Score: 0.0020829707\n",
            "Document: {'data': {'text': 'Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!'}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Rerank the documents\n",
        "results = co.rerank(query=queries_for_search,\n",
        "                    documents=[doc['data']['text'] for doc in retrieved_documents],\n",
        "                    top_n=2,\n",
        "                    model='rerank-english-v3.0')\n",
        "\n",
        "# Display the reranking results\n",
        "for idx, result in enumerate(results.results):\n",
        "    print(f\"Rank: {idx+1}\")\n",
        "    print(f\"Score: {result.relevance_score}\")\n",
        "    print(f\"Document: {retrieved_documents[result.index]}\\n\")\n",
        "\n",
        "reranked_documents = [retrieved_documents[result.index] for result in results.results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITJG4qZy458w"
      },
      "source": [
        "Finally we reach the step that we saw in the earlier `Basic RAG` section.\n",
        "\n",
        "To call the Chat API with RAG, we pass the following parameters. This tells the model to run in RAG-mode and use these documents in its response.\n",
        "\n",
        "- `model` for the model ID\n",
        "- `messages` for the user's query.\n",
        "- `documents` for defining the documents.\n",
        "\n",
        "The response is then generated based on the the query and the documents retrieved.\n",
        "\n",
        "RAG introduces additional objects in the Chat response. One of them is `citations`, which contains details about:\n",
        "- specific text spans from the retrieved documents on which the response is grounded.\n",
        "- the documents referenced in the citations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "t_2okwdp458w",
        "outputId": "ad220e0b-c6cd-474c-af78-39c032cce335",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You can get to know your teammates by joining Slack channels and participating in team-building activities. You will receive an invite via email to join relevant Slack channels. You can also foster team spirit by participating in monthly outings and weekly game nights.\n",
            "\n",
            "CITATIONS:\n",
            "start=38 end=60 text='joining Slack channels' sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Joining Slack Channels: You will receive an invite via email. Be sure to join relevant channels to stay informed and engaged.'})] \n",
            "\n",
            "start=82 end=107 text='team-building activities.' sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'text': 'Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!'})] \n",
            "\n",
            "start=117 end=144 text='receive an invite via email' sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Joining Slack Channels: You will receive an invite via email. Be sure to join relevant channels to stay informed and engaged.'})] \n",
            "\n",
            "start=153 end=177 text='relevant Slack channels.' sources=[DocumentSource(type='document', id='doc:0', document={'id': 'doc:0', 'text': 'Joining Slack Channels: You will receive an invite via email. Be sure to join relevant channels to stay informed and engaged.'})] \n",
            "\n",
            "start=191 end=209 text='foster team spirit' sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'text': 'Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!'})] \n",
            "\n",
            "start=230 end=269 text='monthly outings and weekly game nights.' sources=[DocumentSource(type='document', id='doc:1', document={'id': 'doc:1', 'text': 'Team-Building Activities: We foster team spirit with monthly outings and weekly game nights. Feel free to suggest new activity ideas anytime!'})] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate the response\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=[{'role': 'user', 'content': query}],\n",
        "                   documents=reranked_documents)\n",
        "\n",
        "# Display the response\n",
        "print(response.message.content[0].text)\n",
        "\n",
        "# Display the citations and source documents\n",
        "if response.message.citations:\n",
        "    print(\"\\nCITATIONS:\")\n",
        "    for citation in response.message.citations:\n",
        "        print(citation, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJk5lQe35zN_"
      },
      "source": [
        "# Agents with Tool Use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wJ7Awpn5zOA"
      },
      "source": [
        "Tool use extends the ideas from RAG, where external systems are used to guide the response of an LLM, but by leveraging a much bigger set of tools than what’s possible with RAG. The concept of tool use leverages LLMs' useful feature of being able to act as a reasoning and decision-making engine.\n",
        "\n",
        "While RAG enables applications that can *answer questions*, tool use enables those that can *automate tasks*.\n",
        "\n",
        "Tool use also enables developers to build agentic applications that can take actions, that is, doing both read and write operations on an external system.\n",
        "\n",
        "In this tutorial, you'll learn about:\n",
        "- Creating tools\n",
        "- Tool planning and calling\n",
        "- Tool execution\n",
        "- Response and citation generation\n",
        "- Multi-step tool use\n",
        "\n",
        "You'll learn these by building an onboarding assistant for new hires."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfPrGaeJ5zOB"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To get started, first we need to install the `cohere` library and create a Cohere client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Ka73Hc8z5zOC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# pip install cohere\n",
        "\n",
        "import cohere\n",
        "import json\n",
        "import os\n",
        "\n",
        "co = cohere.ClientV2(api_key=\"1GLAqzgbTVGbUTfUeblk3ByYtQyN0fjvHLbs5fPr\") # Get your free API key: https://dashboard.cohere.com/api-keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPZAFdtE5zOC"
      },
      "source": [
        "## Creating tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je1hqkWj5zOD"
      },
      "source": [
        "The pre-requisite, before we can run a tool use workflow, is to set up the tools. Let's create three tools:\n",
        "- `search_faqs`: A tool for searching the FAQs. For simplicity, we'll not implement any retrieval logic, but we'll simply pass a list of pre-defined documents, which are the FAQ documents we had used in the Text Embeddings section.\n",
        "- `search_emails`: A tool for searching the emails. Same as above, we'll simply pass a list of pre-defined emails from the Reranking section.\n",
        "- `create_calendar_event`: A tool for creating new calendar events. Again, for simplicity, we'll not implement actual event bookings, but will return a mock success event. In practice, we can connect to a calendar service API and implement all the necessary logic here.\n",
        "\n",
        "Here, we are defining a Python function for each tool, but more broadly, the tool can be any function or service that can receive and send objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "v0gWtBY65zOD"
      },
      "outputs": [],
      "source": [
        "# Create the tools\n",
        "def search_faqs(query):\n",
        "    faqs = [\n",
        "        {\"text\": \"Reimbursing Travel Expenses: Easily manage your travel expenses by submitting them through our finance tool. Approvals are prompt and straightforward.\"},\n",
        "        {\"text\": \"Working from Abroad: Working remotely from another country is possible. Simply coordinate with your manager and ensure your availability during core hours.\"}\n",
        "    ]\n",
        "    return  faqs\n",
        "\n",
        "def search_emails(query):\n",
        "    emails = [\n",
        "        {\"from\": \"it@co1t.com\", \"to\": \"david@co1t.com\", \"date\": \"2024-06-24\", \"subject\": \"Setting Up Your IT Needs\", \"text\": \"Greetings! To ensure a seamless start, please refer to the attached comprehensive guide, which will assist you in setting up all your work accounts.\"},\n",
        "        {\"from\": \"john@co1t.com\", \"to\": \"david@co1t.com\", \"date\": \"2024-06-24\", \"subject\": \"First Week Check-In\", \"text\": \"Hello! I hope you're settling in well. Let's connect briefly tomorrow to discuss how your first week has been going. Also, make sure to join us for a welcoming lunch this Thursday at noon—it's a great opportunity to get to know your colleagues!\"}\n",
        "    ]\n",
        "    return  emails\n",
        "\n",
        "def create_calendar_event(date: str, time: str, duration: int):\n",
        "    # You can implement any logic here\n",
        "    return {\"is_success\": True,\n",
        "            \"message\": f\"Created a {duration} hour long event at {time} on {date}\"}\n",
        "\n",
        "functions_map = {\n",
        "    \"search_faqs\": search_faqs,\n",
        "    \"search_emails\": search_emails,\n",
        "    \"create_calendar_event\": create_calendar_event\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvVeQmoL5zOE"
      },
      "source": [
        "The second and final setup step is to define the tool schemas in a format that can be passed to the Chat endpoint. The schema must contain the following fields: `name`, `description`, and `parameters` in the format shown below.\n",
        "\n",
        "This schema informs the LLM about what the tool does, and the LLM decides whether to use a particular tool based on it. Therefore, the more descriptive and specific the schema, the more likely the LLM will make the right tool call decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJvoh-Ik5zOE"
      },
      "source": [
        "Further reading:\n",
        "- [Documentation on parameter types in tool use](https://docs.cohere.com/v2/docs/parameter-types-in-tool-use)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "uLvjMima5zOF"
      },
      "outputs": [],
      "source": [
        "# Define the tools\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"search_faqs\",\n",
        "            \"description\": \"Given a user query, searches a company's frequently asked questions (FAQs) list and returns the most relevant matches to the query.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The query from the user\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"query\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"search_emails\",\n",
        "            \"description\": \"Given a user query, searches a person's emails and returns the most relevant matches to the query.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The query from the user\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"query\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"create_calendar_event\",\n",
        "            \"description\": \"Creates a new calendar event of the specified duration at the specified time and date. A new event cannot be created on the same time as an existing event.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"date\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"the date on which the event starts, formatted as mm/dd/yy\"\n",
        "                    },\n",
        "                    \"time\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"the time of the event, formatted using 24h military time formatting\"\n",
        "                    },\n",
        "                    \"duration\": {\n",
        "                        \"type\": \"number\",\n",
        "                        \"description\": \"the number of hours the event lasts for\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"date\", \"time\", \"duration\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uulxFsaI5zOG"
      },
      "source": [
        "## Tool planning and calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRlU9NM15zOG"
      },
      "source": [
        "We can now run the tool use workflow. We can think of a tool use system as consisting of four components:\n",
        "- The user\n",
        "- The application\n",
        "- The LLM\n",
        "- The tools\n",
        "\n",
        "At its most basic, these four components interact in a workflow through four steps:\n",
        "- **Step 1: Get user message** – The LLM gets the user message (via the application)\n",
        "- **Step 2: Tool planning and calling** – The LLM makes a decision on the tools to call (if any) and generates - the tool calls\n",
        "- **Step 3: Tool execution** - The application executes the tools and the results are sent to the LLM\n",
        "- **Step 4: Response and citation generation** – The LLM generates the response and citations to back to the user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "I_JYxpYI5zOG",
        "outputId": "08ab3b97-1aad-480b-ec34-12fd049cab9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool plan:\n",
            "I will search the emails for messages about getting set up with IT. \n",
            "\n",
            "Tool calls:\n",
            "Tool name: search_emails | Parameters: {\"query\":\"getting setup with IT\"}\n"
          ]
        }
      ],
      "source": [
        "# Create custom system message\n",
        "system_message = \"\"\"## Task and Context\n",
        "You are an assistant who assist new employees of Co1t with their first week. You respond to their questions and assist them with their needs. Today is Monday, June 24, 2024\"\"\"\n",
        "\n",
        "\n",
        "# Step 1: Get user message\n",
        "message = \"Is there any message about getting setup with IT?\"\n",
        "\n",
        "# Add the system and user messages to the chat history\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": message},\n",
        "]\n",
        "\n",
        "# Step 2: Tool planning and calling\n",
        "response = co.chat(model=\"command-r-plus-08-2024\", messages=messages, tools=tools)\n",
        "\n",
        "if response.message.tool_calls:\n",
        "    print(\"Tool plan:\")\n",
        "    print(response.message.tool_plan, \"\\n\")\n",
        "    print(\"Tool calls:\")\n",
        "    for tc in response.message.tool_calls:\n",
        "        print(f\"Tool name: {tc.function.name} | Parameters: {tc.function.arguments}\")\n",
        "\n",
        "    # Append tool calling details to the chat history\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"tool_calls\": response.message.tool_calls,\n",
        "            \"tool_plan\": response.message.tool_plan,\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSQ0-wSA5zOH"
      },
      "source": [
        "Given three tools to choose from, the model is able to pick the right tool (in this case, `search_emails`) based on what the user is asking for.\n",
        "\n",
        "Also, notice that the model first generates a plan about what it should do (\"I will do ...\") before actually generating the tool call(s)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVQ4yjZS5zOH"
      },
      "source": [
        "# Tool execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "kl_Gmqw55zOH",
        "outputId": "e7999a12-c507-443c-b308-0808c672fe80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool results:\n",
            "{'type': 'document', 'document': {'data': '{\"from\": \"it@co1t.com\", \"to\": \"david@co1t.com\", \"date\": \"2024-06-24\", \"subject\": \"Setting Up Your IT Needs\", \"text\": \"Greetings! To ensure a seamless start, please refer to the attached comprehensive guide, which will assist you in setting up all your work accounts.\"}'}}\n",
            "{'type': 'document', 'document': {'data': '{\"from\": \"john@co1t.com\", \"to\": \"david@co1t.com\", \"date\": \"2024-06-24\", \"subject\": \"First Week Check-In\", \"text\": \"Hello! I hope you\\'re settling in well. Let\\'s connect briefly tomorrow to discuss how your first week has been going. Also, make sure to join us for a welcoming lunch this Thursday at noon\\\\u2014it\\'s a great opportunity to get to know your colleagues!\"}'}}\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Tool execution\n",
        "for tc in response.message.tool_calls:\n",
        "    tool_result = functions_map[tc.function.name](**json.loads(tc.function.arguments))\n",
        "    tool_content = []\n",
        "    for idx, data in enumerate(tool_result):\n",
        "        tool_content.append({\"type\": \"document\", \"document\": {\"data\": json.dumps(data)}})\n",
        "        # Optional: add an \"id\" field in the \"document\" object, otherwise IDs are auto-generated\n",
        "    # Append tool results to the chat history\n",
        "    messages.append({\"role\": \"tool\", \"tool_call_id\": tc.id, \"content\": tool_content})\n",
        "\n",
        "print(\"Tool results:\")\n",
        "for result in tool_content:\n",
        "    print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdW60-bD5zOH"
      },
      "source": [
        "## Response and citation generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "8jnjsscF5zOH",
        "outputId": "16145a75-994b-4a4a-c12e-ffc28090d72a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "Yes, there is an email from it@co1t.com with the subject 'Setting Up Your IT Needs'. It includes a comprehensive guide to setting up your work accounts.\n",
            "==================================================\n",
            "\n",
            "CITATIONS:\n",
            "start=28 end=39 text='it@co1t.com' sources=[ToolSource(type='tool', id='search_emails_r142nnch4wed:0', tool_output={'date': '2024-06-24', 'from': 'it@co1t.com', 'subject': 'Setting Up Your IT Needs', 'text': 'Greetings! To ensure a seamless start, please refer to the attached comprehensive guide, which will assist you in setting up all your work accounts.', 'to': 'david@co1t.com'})] \n",
            "\n",
            "start=57 end=83 text=\"'Setting Up Your IT Needs'\" sources=[ToolSource(type='tool', id='search_emails_r142nnch4wed:0', tool_output={'date': '2024-06-24', 'from': 'it@co1t.com', 'subject': 'Setting Up Your IT Needs', 'text': 'Greetings! To ensure a seamless start, please refer to the attached comprehensive guide, which will assist you in setting up all your work accounts.', 'to': 'david@co1t.com'})] \n",
            "\n",
            "start=99 end=152 text='comprehensive guide to setting up your work accounts.' sources=[ToolSource(type='tool', id='search_emails_r142nnch4wed:0', tool_output={'date': '2024-06-24', 'from': 'it@co1t.com', 'subject': 'Setting Up Your IT Needs', 'text': 'Greetings! To ensure a seamless start, please refer to the attached comprehensive guide, which will assist you in setting up all your work accounts.', 'to': 'david@co1t.com'})] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Response and citation generation\n",
        "response = co.chat(\n",
        "    model=\"command-r-plus-08-2024\",\n",
        "    messages=messages,\n",
        "    tools=tools\n",
        ")\n",
        "\n",
        "# Append assistant response to the chat history\n",
        "messages.append({\"role\": \"assistant\", \"content\": response.message.content[0].text})\n",
        "\n",
        "# Print final response\n",
        "print(\"Response:\")\n",
        "print(response.message.content[0].text)\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Print citations (if any)\n",
        "if response.message.citations:\n",
        "    print(\"\\nCITATIONS:\")\n",
        "    for citation in response.message.citations:\n",
        "        print(citation, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFKV-tgA5zOH"
      },
      "source": [
        "# Multi-step tool use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5JzxOU05zOH"
      },
      "source": [
        "The model can execute more complex tasks in tool use – tasks that require tool calls to happen in a sequence. This is referred to as \"multi-step\" tool use.\n",
        "\n",
        "Let's create a function to called `run_assistant` to implement these steps, and along the way, print out the key events and messages. Optionally, this function also accepts the chat history as an argument to keep the state in a multi-turn conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Us3L-nng5zOI"
      },
      "outputs": [],
      "source": [
        "model = \"command-r-plus-08-2024\"\n",
        "\n",
        "system_message = \"\"\"## Task and Context\n",
        "You are an assistant who assists new employees of Co1t with their first week. You respond to their questions and assist them with their needs. Today is Monday, June 24, 2024\"\"\"\n",
        "\n",
        "\n",
        "def run_assistant(query, messages=None):\n",
        "    if messages is None:\n",
        "        messages = []\n",
        "\n",
        "    if \"system\" not in {m.get(\"role\") for m in messages}:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "\n",
        "    # Step 1: get user message\n",
        "    print(f\"Question:\\n{query}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": query})\n",
        "\n",
        "    # Step 2: Generate tool calls (if any)\n",
        "    response = co.chat(model=model, messages=messages, tools=tools)\n",
        "\n",
        "    while response.message.tool_calls:\n",
        "\n",
        "        print(\"Tool plan:\")\n",
        "        print(response.message.tool_plan, \"\\n\")\n",
        "        print(\"Tool calls:\")\n",
        "        for tc in response.message.tool_calls:\n",
        "            print(\n",
        "                f\"Tool name: {tc.function.name} | Parameters: {tc.function.arguments}\"\n",
        "            )\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"tool_calls\": response.message.tool_calls,\n",
        "                \"tool_plan\": response.message.tool_plan,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Step 3: Get tool results\n",
        "        for idx, tc in enumerate(response.message.tool_calls):\n",
        "            tool_result = functions_map[tc.function.name](\n",
        "                **json.loads(tc.function.arguments)\n",
        "            )\n",
        "            tool_content = []\n",
        "            for idx, data in enumerate(tool_result):\n",
        "                tool_content.append({\"type\": \"document\", \"document\": {\"data\": json.dumps(data)}})\n",
        "                # Optional: add an \"id\" field in the \"document\" object, otherwise IDs are auto-generated\n",
        "            messages.append(\n",
        "                {\"role\": \"tool\", \"tool_call_id\": tc.id, \"content\": tool_content}\n",
        "            )\n",
        "\n",
        "        # Step 4: Generate response and citations\n",
        "        response = co.chat(model=model, messages=messages, tools=tools)\n",
        "\n",
        "    messages.append({\"role\": \"assistant\", \"content\": response.message.content[0].text})\n",
        "\n",
        "    # Print final response\n",
        "    print(\"Response:\")\n",
        "    print(response.message.content[0].text)\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Print citations (if any)\n",
        "    if response.message.citations:\n",
        "        print(\"\\nCITATIONS:\")\n",
        "        for citation in response.message.citations:\n",
        "            print(citation, \"\\n\")\n",
        "\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLIW_3h35zOI"
      },
      "source": [
        "To illustrate the concept of multi-step tool user, let's ask the assistant to block time for any lunch invites received in the email.\n",
        "\n",
        "This requires tasks to happen over multiple steps in a sequence. Here, we see the assistant running these steps:\n",
        "- First, it calls the `search_emails` tool to find any lunch invites, which it found one.\n",
        "- Next, it calls the `create_calendar_event` tool to create an event to block the person's calendar on the day mentioned by the email.\n",
        "\n",
        "This is also an example of tool use enabling a write operation instead of just a read operation that we saw with RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "i3Mcxtud5zOI",
        "outputId": "55779002-5d41-41d8-cb61-d23a371a01a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            "Can you check if there are any lunch invites, and for those days, create a one-hour event on my calendar at 12PM.\n",
            "==================================================\n",
            "Tool plan:\n",
            "I will search the user's emails for lunch invites. Then, I will create a one-hour event on the user's calendar at 12PM for each day that the user has a lunch invite. \n",
            "\n",
            "Tool calls:\n",
            "Tool name: search_emails | Parameters: {\"query\":\"lunch invites\"}\n",
            "==================================================\n",
            "Tool plan:\n",
            "I found one lunch invite for Thursday at noon. I will now create a one-hour event on the user's calendar for Thursday at 12PM. \n",
            "\n",
            "Tool calls:\n",
            "Tool name: create_calendar_event | Parameters: {\"date\":\"06/27/24\",\"duration\":1,\"time\":\"12:00\"}\n",
            "==================================================\n",
            "Response:\n",
            "I found one lunch invite for Thursday at noon. I have created a one-hour event on your calendar for Thursday at 12PM.\n",
            "==================================================\n",
            "\n",
            "CITATIONS:\n",
            "start=29 end=46 text='Thursday at noon.' sources=[ToolSource(type='tool', id='search_emails_zdas36mjahhr:1', tool_output={'date': '2024-06-24', 'from': 'john@co1t.com', 'subject': 'First Week Check-In', 'text': \"Hello! I hope you're settling in well. Let's connect briefly tomorrow to discuss how your first week has been going. Also, make sure to join us for a welcoming lunch this Thursday at noon—it's a great opportunity to get to know your colleagues!\", 'to': 'david@co1t.com'})] \n",
            "\n",
            "start=64 end=117 text='one-hour event on your calendar for Thursday at 12PM.' sources=[ToolSource(type='tool', id='create_calendar_event_81wynsthc4xg:0', tool_output={'content': '\"is_success\"'}), ToolSource(type='tool', id='create_calendar_event_81wynsthc4xg:1', tool_output={'content': '\"message\"'})] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "messages = run_assistant(\"Can you check if there are any lunch invites, and for those days, create a one-hour event on my calendar at 12PM.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeYY1I7t5zOI"
      },
      "source": [
        "In this tutorial, you learned about:\n",
        "- How to create tools\n",
        "- How tool planning and calling happens\n",
        "- How tool execution happens\n",
        "- How to generate the response and citations\n",
        "- How to run tool use in a multi-step scenario\n",
        "\n",
        "And that concludes our 7-part Cohere tutorial. We hope that they have provided you with a foundational understanding of the Cohere API, the available models and endpoints, and the types of use cases that you can build with them.\n",
        "\n",
        "To continue your learning, check out:\n",
        "- [LLM University - A range of courses and step-by-step guides to help you start building](https://cohere.com/llmu)\n",
        "- [Cookbooks - A collection of basic to advanced example applications](https://docs.cohere.com/page/cookbooks)\n",
        "- [Cohere's documentation](https://docs.cohere.com/docs/the-cohere-platform)\n",
        "- [The Cohere API reference](https://docs.cohere.com/reference/about)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}